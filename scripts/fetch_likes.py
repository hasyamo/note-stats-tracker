"""
noteã‚¹ã‚­ãƒ‡ãƒ¼ã‚¿è“„ç©ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¨˜äº‹ã”ã¨ã®ã‚¹ã‚­è©³ç´°ï¼ˆèª°ãŒã„ã¤ã‚¹ã‚­ã—ãŸã‹ï¼‰ã‚’CSVã«è“„ç©ã™ã‚‹
v3 likes APIã¯å…¬é–‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆèªè¨¼ä¸è¦ï¼‰
"""

import os
import csv
import json
import time
import sys
from datetime import timezone, timedelta
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError

# Windowsç’°å¢ƒã§ã®Unicodeå‡ºåŠ›å¯¾å¿œ
if sys.stdout.encoding and sys.stdout.encoding.lower() not in ("utf-8", "utf8"):
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")

BASE_URL = "https://note.com"
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
ARTICLES_CSV = os.path.join(DATA_DIR, "articles.csv")
LIKES_CSV = os.path.join(DATA_DIR, "likes.csv")

JST = timezone(timedelta(hours=9))

LIKES_API_SIZE = 50
SLEEP_BETWEEN_ARTICLES = 1.5
SLEEP_BETWEEN_PAGES = 1.0

LIKES_HEADER = ["note_key", "like_user_id", "like_username",
                "like_user_urlname", "liked_at", "follower_count"]


def fetch_likes_api(note_key, start=0, size=LIKES_API_SIZE):
    """1ãƒšãƒ¼ã‚¸åˆ†ã®ã‚¹ã‚­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯None"""
    url = f"{BASE_URL}/api/v3/notes/{note_key}/likes?start={start}&size={size}"
    req = Request(url)
    req.add_header("Accept", "application/json, text/plain, */*")
    req.add_header("User-Agent", "Mozilla/5.0")
    req.add_header("Referer", "https://note.com/")

    try:
        with urlopen(req, timeout=30) as res:
            return json.loads(res.read().decode("utf-8"))
    except HTTPError as e:
        print(f"  âš  API HTTPã‚¨ãƒ©ãƒ¼ ({note_key}, start={start}): {e.code}")
        return None
    except URLError as e:
        print(f"  âš  APIé€šä¿¡ã‚¨ãƒ©ãƒ¼ ({note_key}): {e.reason}")
        return None


def fetch_all_likes_for_article(note_key):
    """1è¨˜äº‹ã®å…¨ã‚¹ã‚­ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
    æ³¨æ„: APIã®is_last_pageã¯ä¿¡é ¼ã§ããªã„ï¼ˆå¸¸ã«Falseã‚’è¿”ã™å ´åˆãŒã‚ã‚‹ï¼‰
    like_countãƒ™ãƒ¼ã‚¹ã®çµ‚äº†åˆ¤å®š + é‡è¤‡æ¤œçŸ¥ã§å®‰å…¨ã«åœæ­¢ã™ã‚‹
    """
    all_likes = []
    seen_ids = set()
    start = 0
    total_count = None

    while True:
        resp = fetch_likes_api(note_key, start)
        if resp is None:
            break

        data = resp.get("data", {})
        likes = data.get("likes", [])

        if total_count is None:
            total_count = data.get("extra_fields", {}).get("like_count", 0)

        if not likes:
            break

        new_in_page = 0
        for like in likes:
            user = like.get("user", {})
            user_id = str(user.get("id", ""))
            if user_id in seen_ids:
                continue
            seen_ids.add(user_id)
            new_in_page += 1
            all_likes.append({
                "note_key": note_key,
                "like_user_id": user_id,
                "like_username": user.get("nickname", ""),
                "like_user_urlname": user.get("urlname", ""),
                "liked_at": like.get("created_at", ""),
                "follower_count": user.get("follower_count", 0),
            })

        # çµ‚äº†æ¡ä»¶: å…¨ä»¶å–å¾—æ¸ˆã¿ or ãƒšãƒ¼ã‚¸å†…ã§æ–°è¦ãªã—ï¼ˆAPIãŒåŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¦ã„ã‚‹ï¼‰
        if len(all_likes) >= total_count or new_in_page == 0:
            break

        start += LIKES_API_SIZE
        time.sleep(SLEEP_BETWEEN_PAGES)

    if total_count is not None and len(all_likes) < total_count:
        print(f"    âš  {note_key}: {total_count}ä»¶ä¸­{len(all_likes)}ä»¶ã®ã¿å–å¾—ï¼ˆAPIãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³åˆ¶é™ï¼‰")

    return all_likes


def load_existing_likes():
    """likes.csvã‹ã‚‰æ—¢å­˜ã®(note_key, like_user_id)ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰"""
    if not os.path.exists(LIKES_CSV):
        return set(), 0

    existing = set()
    count = 0
    try:
        with open(LIKES_CSV, newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if header is None:
                return set(), 0
            for row in reader:
                if len(row) >= 2:
                    existing.add((row[0], row[1]))
                    count += 1
    except OSError as e:
        print(f"  âš  likes.csvèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return set(), 0

    return existing, count


def load_articles_csv():
    """articles.csvã‚’èª­ã¿ã€æ—¥ä»˜ã”ã¨ã®{note_key: like_count}ã‚’è¿”ã™"""
    if not os.path.exists(ARTICLES_CSV):
        print("ğŸš¨ articles.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {}, []

    with open(ARTICLES_CSV, newline="", encoding="utf-8") as f:
        reader = list(csv.reader(f))

    if len(reader) < 2:
        print("ğŸš¨ articles.csv ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return {}, []

    header = reader[0]
    try:
        date_idx = header.index("date")
        key_idx = header.index("key")
        like_count_idx = header.index("like_count")
    except ValueError as e:
        print(f"ğŸš¨ articles.csv ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“: {e}")
        return {}, []

    articles_by_date = {}
    for row in reader[1:]:
        if len(row) <= max(date_idx, key_idx, like_count_idx):
            continue
        date_str = row[date_idx]
        note_key = row[key_idx]
        try:
            like_count = int(row[like_count_idx])
        except ValueError:
            like_count = 0
        if date_str not in articles_by_date:
            articles_by_date[date_str] = {}
        articles_by_date[date_str][note_key] = like_count

    sorted_dates = sorted(articles_by_date.keys())
    return articles_by_date, sorted_dates


def find_articles_with_new_likes(articles_by_date, sorted_dates):
    """æœ€æ–°2æ—¥åˆ†ã‚’æ¯”è¼ƒã—ã€ã‚¹ã‚­å¢—åŠ ã—ãŸè¨˜äº‹ã®note_keyãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    if len(sorted_dates) < 2:
        return None  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    latest = sorted_dates[-1]
    previous = sorted_dates[-2]

    latest_data = articles_by_date[latest]
    previous_data = articles_by_date[previous]

    changed = []
    for note_key, like_count in latest_data.items():
        prev_count = previous_data.get(note_key, 0)
        if like_count > prev_count:
            changed.append(note_key)

    return changed


def append_likes_csv(new_likes):
    """æ–°è¦ã‚¹ã‚­ã‚’likes.csvã«è¿½è¨˜"""
    if not new_likes:
        return

    file_exists = os.path.exists(LIKES_CSV)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
    write_header = not file_exists
    if file_exists:
        with open(LIKES_CSV, newline="", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                write_header = True

    with open(LIKES_CSV, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(LIKES_HEADER)
        for like in new_likes:
            writer.writerow([
                like["note_key"],
                like["like_user_id"],
                like["like_username"],
                like["like_user_urlname"],
                like["liked_at"],
                like["follower_count"],
            ])


def main():
    try:
        print("=== note likes collector ===")

        articles_by_date, sorted_dates = load_articles_csv()
        if not articles_by_date:
            print("è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚çµ‚äº†ã—ã¾ã™")
            return

        print(f"articles.csv: {len(sorted_dates)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ{sorted_dates[0]} ã€œ {sorted_dates[-1]}ï¼‰")

        existing_set, existing_count = load_existing_likes()
        baseline = existing_count == 0

        if baseline:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰: å…¨è¨˜äº‹ã®ã‚¹ã‚­ã‚’ä¸€æ‹¬å–å¾—
            latest = sorted_dates[-1]
            all_keys = list(articles_by_date[latest].keys())
            print(f"\nåˆå›ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¾—: {len(all_keys)}è¨˜äº‹")

            all_new_likes = []
            for i, note_key in enumerate(all_keys, 1):
                likes = fetch_all_likes_for_article(note_key)
                new = [l for l in likes if (l["note_key"], l["like_user_id"]) not in existing_set]
                all_new_likes.extend(new)
                for l in new:
                    existing_set.add((l["note_key"], l["like_user_id"]))
                print(f"  {i}/{len(all_keys)} {note_key}: {len(likes)}ä»¶å–å¾—, {len(new)}ä»¶æ–°è¦")
                if i < len(all_keys):
                    time.sleep(SLEEP_BETWEEN_ARTICLES)

            os.makedirs(DATA_DIR, exist_ok=True)
            append_likes_csv(all_new_likes)
            print(f"\nå®Œäº†: {len(all_new_likes)}ä»¶ã®ã‚¹ã‚­ã‚’è¨˜éŒ²")

        else:
            # æ—¥æ¬¡å·®åˆ†ãƒ¢ãƒ¼ãƒ‰
            changed = find_articles_with_new_likes(articles_by_date, sorted_dates)

            if changed is None:
                # æ—¥ä»˜ãŒ1ã¤ã—ã‹ãªã„å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰±ã„ã ãŒæ—¢ã«ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šâ†’ã‚¹ã‚­ãƒƒãƒ—
                print("å·®åˆ†æ¯”è¼ƒã«å¿…è¦ãªæ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return

            if not changed:
                print("\nã‚¹ã‚­æ•°ã®å¤‰åŒ–ãªã—")
                return

            print(f"\nã‚¹ã‚­å¢—åŠ : {len(changed)}è¨˜äº‹ã‚’å–å¾—")

            all_new_likes = []
            for i, note_key in enumerate(changed, 1):
                likes = fetch_all_likes_for_article(note_key)
                new = [l for l in likes if (l["note_key"], l["like_user_id"]) not in existing_set]
                all_new_likes.extend(new)
                for l in new:
                    existing_set.add((l["note_key"], l["like_user_id"]))

                total = len(likes)
                added = len(new)
                print(f"  {i}/{len(changed)} {note_key}: {total}ä»¶ä¸­{added}ä»¶æ–°è¦")

                if i < len(changed):
                    time.sleep(SLEEP_BETWEEN_ARTICLES)

            os.makedirs(DATA_DIR, exist_ok=True)
            append_likes_csv(all_new_likes)
            print(f"\nå®Œäº†: æ–°è¦{len(all_new_likes)}ä»¶ã®ã‚¹ã‚­ã‚’è¨˜éŒ²")

    except Exception as e:
        print(f"\nã‚¹ã‚­å–å¾—ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(0)


if __name__ == "__main__":
    main()
