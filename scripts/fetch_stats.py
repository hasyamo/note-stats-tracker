"""
noteè¨˜äº‹ãƒ‡ãƒ¼ã‚¿æ—¥æ¬¡å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GitHub Actionsã§æ¯æ—¥å®Ÿè¡Œã—ã€è¨˜äº‹ã”ã¨ã®ãƒ“ãƒ¥ãƒ¼ãƒ»ã‚¹ã‚­ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã‚’CSVã«è“„ç©ã™ã‚‹
"""

import os
import csv
import json
import time
import sys
from datetime import datetime, timezone, timedelta
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
from pathlib import Path

# Windowsç’°å¢ƒã§ã®Unicodeå‡ºåŠ›å¯¾å¿œ
if sys.stdout.encoding and sys.stdout.encoding.lower() not in ("utf-8", "utf8"):
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")


def load_dotenv():
    """ç°¡æ˜“.envãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆpython-dotenvä¸è¦ï¼‰"""
    env_path = Path(__file__).resolve().parent.parent / ".env"
    if not env_path.exists():
        print(f"[dotenv] .envãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {env_path}")
        return
    print(f"[dotenv] .envãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {env_path}")
    loaded = []
    with open(env_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "=" in line:
                key, _, value = line.partition("=")
                key = key.strip()
                value = value.strip().strip('"').strip("'")
                if key not in os.environ:
                    os.environ[key] = value
                    loaded.append(key)
                else:
                    print(f"[dotenv] {key}: ç’°å¢ƒå¤‰æ•°ãŒæ—¢ã«è¨­å®šæ¸ˆã¿ï¼ˆ.envã®å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    for key in loaded:
        val = os.environ[key]
        if key == "NOTE_COOKIE":
            # Cookieã¯å…ˆé ­20æ–‡å­—ã ã‘è¡¨ç¤º
            print(f"[dotenv] {key} = {val[:20]}...ï¼ˆ{len(val)}æ–‡å­—ï¼‰")
        else:
            print(f"[dotenv] {key} = {val}")


load_dotenv()

# è¨­å®š
NOTE_COOKIE = os.environ.get("NOTE_COOKIE", "")
NOTE_USERNAME = os.environ.get("NOTE_USERNAME", "")
COOKIE_SET_DATE = os.environ.get("COOKIE_SET_DATE", "")  # YYYY-MM-DDå½¢å¼
BASE_URL = "https://note.com"
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")

JST = timezone(timedelta(hours=9))


def get_today_jst():
    return datetime.now(JST).strftime("%Y-%m-%d")


def check_cookie_expiry():
    """Cookieã®æœŸé™ãŒè¿‘ã¥ã„ã¦ã„ãŸã‚‰è­¦å‘Š"""
    if not COOKIE_SET_DATE:
        print("âš  COOKIE_SET_DATE ãŒæœªè¨­å®šã§ã™ã€‚æœŸé™ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    try:
        set_date = datetime.strptime(COOKIE_SET_DATE, "%Y-%m-%d").replace(tzinfo=JST)
        days_elapsed = (datetime.now(JST) - set_date).days
        days_remaining = 90 - days_elapsed
        if days_remaining <= 0:
            print(f"ğŸš¨ CookieãŒæœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆè¨­å®šã‹ã‚‰{days_elapsed}æ—¥çµŒéï¼‰")
        elif days_remaining <= 10:
            print(f"âš  CookieæœŸé™ã¾ã§ã‚ã¨ç´„{days_remaining}æ—¥ã§ã™ï¼æ—©ã‚ã«æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
        else:
            print(f"âœ“ CookieæœŸé™: ã‚ã¨ç´„{days_remaining}æ—¥")
    except ValueError:
        print(f"âš  COOKIE_SET_DATE ã®å½¢å¼ãŒä¸æ­£ã§ã™: {COOKIE_SET_DATE}")


def validate_cookie():
    """Cookieå€¤ã®åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
    if not NOTE_COOKIE:
        print("ğŸš¨ NOTE_COOKIE ãŒç©ºã§ã™ã€‚")
        print("   â†’ .envã¾ãŸã¯ãƒªãƒã‚¸ãƒˆãƒªã®Secretsã«Cookieã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    # æœ€ä½é™ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    if "=" not in NOTE_COOKIE:
        print("ğŸš¨ NOTE_COOKIE ã®å½¢å¼ãŒä¸æ­£ã§ã™ï¼ˆkey=valueå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚")
        print(f"   â†’ ç¾åœ¨ã®å€¤: {NOTE_COOKIE[:30]}...")
        sys.exit(1)

    # ã‚ˆãã‚ã‚‹é–“é•ã„: .envã®ã‚­ãƒ¼åã”ã¨å…¥ã‚Œã¦ã—ã¾ã†
    if NOTE_COOKIE.startswith("NOTE_COOKIE="):
        print("ğŸš¨ NOTE_COOKIE ã®å€¤ã« 'NOTE_COOKIE=' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        print("   â†’ .envã«ã¯ NOTE_COOKIE=å€¤ ã®å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼ˆå€¤ã ã‘ã‚’è¨­å®šï¼‰ã€‚")
        sys.exit(1)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³Cookieã®é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆnoteã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯é€šå¸¸æ•°ç™¾æ–‡å­—ä»¥ä¸Šï¼‰
    if len(NOTE_COOKIE) < 50:
        print(f"âš  NOTE_COOKIE ãŒçŸ­ã™ãã¾ã™ï¼ˆ{len(NOTE_COOKIE)}æ–‡å­—ï¼‰ã€‚")
        print("   â†’ ãƒ–ãƒ©ã‚¦ã‚¶ã®DevToolsã‹ã‚‰å®Œå…¨ãªCookieãƒ˜ãƒƒãƒ€ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„ã€‚")
        print("   â†’ è¤‡æ•°ã®CookieãŒã‚»ãƒŸã‚³ãƒ­ãƒ³åŒºåˆ‡ã‚Šã§å«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚")

    print(f"[debug] Cookieå…ˆé ­: {NOTE_COOKIE[:40]}...")
    print(f"[debug] Cookieé•·: {len(NOTE_COOKIE)}æ–‡å­—")


def verify_auth():
    """APIå‘¼ã³å‡ºã—å‰ã«èªè¨¼ãŒé€šã‚‹ã‹ç¢ºèª"""
    print("\nğŸ”‘ èªè¨¼ãƒã‚§ãƒƒã‚¯ä¸­...")
    url = f"{BASE_URL}/api/v1/stats/pv?filter=all&page=1&sort=pv"
    req = Request(url)
    req.add_header("Cookie", NOTE_COOKIE)
    req.add_header("User-Agent", "note-stats-tracker")

    try:
        with urlopen(req) as res:
            body = json.loads(res.read().decode("utf-8"))
            if "data" in body and "note_stats" in body["data"]:
                print("âœ“ èªè¨¼OKï¼ˆstats APIã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã—ãŸï¼‰")
                return True
            else:
                print("âš  APIã¯å¿œç­”ã—ã¾ã—ãŸãŒã€stats ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                print(f"   â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ¼: {list(body.keys())}")
                print("   â†’ CookieãŒç„¡åŠ¹ã‹ã€ãƒ­ã‚°ã‚¤ãƒ³ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                print("\nğŸ’¡ å¯¾å‡¦æ³•:")
                print("   1. ãƒ–ãƒ©ã‚¦ã‚¶ã§note.comã«ãƒ­ã‚°ã‚¤ãƒ³")
                print("   2. DevTools(F12) â†’ Network â†’ ä»»æ„ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
                print("   3. Request Headersã® 'Cookie' ã‚’ã™ã¹ã¦ã‚³ãƒ”ãƒ¼")
                print("   4. .env ã® NOTE_COOKIE ã«ãƒšãƒ¼ã‚¹ãƒˆ")
                sys.exit(1)
    except HTTPError as e:
        print(f"ğŸš¨ èªè¨¼ãƒã‚§ãƒƒã‚¯å¤±æ•—: HTTP {e.code}")
        if e.code in (401, 403):
            print("   â†’ CookieãŒç„¡åŠ¹ã§ã™ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            print("   1. ãƒ–ãƒ©ã‚¦ã‚¶ã§note.comã«ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã‹")
            print("   2. DevToolsã‹ã‚‰Cookieãƒ˜ãƒƒãƒ€å…¨ä½“ã‚’ã‚³ãƒ”ãƒ¼ã—ãŸã‹")
            print("   3. _note_session_v5 ã ã‘ã§ãªãã€å…¨CookieãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™")
        try:
            error_body = e.read().decode("utf-8")
            print(f"   â†’ ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {error_body[:200]}")
        except Exception:
            pass
        sys.exit(1)
    except URLError as e:
        print(f"âœ— é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e.reason}")
        sys.exit(1)


def fetch_api(path):
    """noteã®APIã‚’å©ã"""
    url = f"{BASE_URL}{path}"
    req = Request(url)
    req.add_header("Cookie", NOTE_COOKIE)
    req.add_header("User-Agent", "note-stats-tracker")

    try:
        with urlopen(req) as res:
            if res.status != 200:
                print(f"âœ— HTTPã‚¨ãƒ©ãƒ¼: {res.status}")
                sys.exit(1)
            return json.loads(res.read().decode("utf-8"))
    except HTTPError as e:
        if e.code in (401, 403):
            print(f"ğŸš¨ èªè¨¼ã‚¨ãƒ©ãƒ¼({e.code}): CookieãŒæœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            print("   â†’ ãƒªãƒã‚¸ãƒˆãƒªã®Secretsã§NOTE_COOKIEã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
        else:
            print(f"âœ— HTTPã‚¨ãƒ©ãƒ¼: {e.code}")
        sys.exit(1)
    except URLError as e:
        print(f"âœ— é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e.reason}")
        sys.exit(1)


def fetch_all_articles():
    """å…¨è¨˜äº‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    all_notes = []
    page = 1

    while True:
        print(f"  ãƒšãƒ¼ã‚¸ {page} å–å¾—ä¸­...")
        data = fetch_api(f"/api/v1/stats/pv?filter=all&page={page}&sort=pv")

        if "data" not in data or "note_stats" not in data["data"]:
            print("ğŸš¨ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚CookieãŒç„¡åŠ¹ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            sys.exit(1)

        stats = data["data"]
        all_notes.extend(stats["note_stats"])

        if stats.get("last_page", True):
            break

        page += 1
        time.sleep(1)  # APIè² è·è»½æ¸›

    total_pv = stats.get("total_pv", 0)
    total_like = stats.get("total_like", 0)
    total_comment = stats.get("total_comment", 0)

    print(f"  â†’ {len(all_notes)}è¨˜äº‹å–å¾—å®Œäº†ï¼ˆç·PV: {total_pv}, ç·ã‚¹ã‚­: {total_like}ï¼‰")

    return all_notes, total_pv, total_like, total_comment


def fetch_follower_count():
    """ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—"""
    if not NOTE_USERNAME:
        print("âš  NOTE_USERNAME ãŒæœªè¨­å®šã§ã™ã€‚ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return None

    data = fetch_api(f"/api/v2/creators/{NOTE_USERNAME}")
    follower_count = data.get("data", {}).get("followerCount")
    if follower_count is not None:
        print(f"  â†’ ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {follower_count}")
    return follower_count


def load_dates_cache():
    """v3 APIã®æ—¥æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€"""
    cache_path = os.path.join(DATA_DIR, "v3_dates_cache.json")
    if not os.path.exists(cache_path):
        return {}
    try:
        with open(cache_path, encoding="utf-8") as f:
            cache = json.load(f)
        # æ—§å½¢å¼ï¼ˆå€¤ãŒæ–‡å­—åˆ—ï¼‰ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ–°å½¢å¼ã«å¤‰æ›
        migrated = {}
        for k, v in cache.items():
            if isinstance(v, str):
                migrated[k] = {"published_at": v, "created_at": "", "updated_at": "", "fetched_at": ""}
            else:
                migrated[k] = v
        return migrated
    except (json.JSONDecodeError, OSError):
        print("âš  v3_dates_cache.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
        return {}


def save_dates_cache(cache):
    """v3 APIã®æ—¥æ™‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã™ã‚‹"""
    cache_path = os.path.join(DATA_DIR, "v3_dates_cache.json")
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


def _is_cache_stale(entry, today_str):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒ7æ—¥ä»¥ä¸Šå¤ã„ã‹åˆ¤å®š"""
    fetched_at = entry.get("fetched_at", "")
    if not fetched_at:
        return True
    try:
        fetched = datetime.strptime(fetched_at, "%Y-%m-%d")
        today = datetime.strptime(today_str, "%Y-%m-%d")
        return (today - fetched).days >= 7
    except ValueError:
        return True


def fetch_note_detail(note_key):
    """v3 APIã‹ã‚‰è¨˜äº‹ã®æ—¥æ™‚æƒ…å ±ã‚’å–å¾—ã™ã‚‹ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºè¾æ›¸ã‚’è¿”ã™ï¼‰"""
    url = f"{BASE_URL}/api/v3/notes/{note_key}"
    req = Request(url)
    req.add_header("Cookie", NOTE_COOKIE)
    req.add_header("User-Agent", "note-stats-tracker")

    try:
        with urlopen(req) as res:
            body = json.loads(res.read().decode("utf-8"))
            data = body.get("data", {})
            published_at = ""
            for key in ("published_at", "publish_at", "first_published_at"):
                if data.get(key):
                    published_at = data[key]
                    break
            return {
                "published_at": published_at,
                "created_at": data.get("created_at", ""),
                "updated_at": data.get("updated_at", ""),
            }
    except (HTTPError, URLError) as e:
        print(f"    âš  v3 API ã‚¨ãƒ©ãƒ¼ ({note_key}): {e}")
        return {"published_at": "", "created_at": "", "updated_at": ""}


def _calc_age_days(today_str, published_at):
    """å–å¾—æ—¥ã¨published_atã®å·®åˆ†æ—¥æ•°ã‚’è¨ˆç®—"""
    if not published_at:
        return ""
    try:
        pub_date = datetime.fromisoformat(published_at).astimezone(JST).date()
        today_date = datetime.strptime(today_str, "%Y-%m-%d").date()
        return (today_date - pub_date).days
    except (ValueError, TypeError):
        return ""


def fetch_note_dates(articles, today_str):
    """å…¨è¨˜äº‹ã®æ—¥æ™‚æƒ…å ±ã‚’å–å¾—ã™ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ã€7æ—¥çµŒéã§å†å–å¾—ï¼‰"""
    cache = load_dates_cache()
    fetched = 0

    for note in articles:
        note_key = note["key"]
        entry = cache.get(note_key)
        if entry and not _is_cache_stale(entry, today_str):
            note["published_at"] = entry["published_at"]
            note["created_at"] = entry["created_at"]
            note["updated_at"] = entry["updated_at"]
        else:
            dates = fetch_note_detail(note_key)
            note["published_at"] = dates["published_at"]
            note["created_at"] = dates["created_at"]
            note["updated_at"] = dates["updated_at"]
            cache[note_key] = {**dates, "fetched_at": today_str}
            fetched += 1
            if fetched % 10 == 0:
                print(f"    {fetched}ä»¶å–å¾—æ¸ˆã¿...")
            time.sleep(0.2)
        note["age_days"] = _calc_age_days(today_str, note["published_at"])

    cached = len(articles) - fetched
    print(f"  â†’ {len(articles)}è¨˜äº‹ä¸­ {fetched}ä»¶ã‚’v3 APIã‹ã‚‰å–å¾—ï¼ˆ{cached}ä»¶ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")

    os.makedirs(DATA_DIR, exist_ok=True)
    save_dates_cache(cache)
    return articles


def _remove_rows_by_date(filepath, date_str):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŒ‡å®šæ—¥ä»˜ã®è¡Œã‚’é™¤å»ã—ã€æ®‹ã‚Šã®è¡Œã‚’è¿”ã™"""
    if not os.path.exists(filepath):
        return [], None
    with open(filepath, newline="", encoding="utf-8") as f:
        reader = list(csv.reader(f))
    if not reader:
        return [], None
    header = reader[0]
    kept = [row for row in reader[1:] if row[0] != date_str]
    removed = len(reader) - 1 - len(kept)
    if removed > 0:
        print(f"  â†’ {date_str} ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿{removed}è¡Œã‚’ä¸Šæ›¸ãã—ã¾ã™")
    return kept, header


def save_articles_csv(today, articles):
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜ï¼ˆåŒæ—¥ãƒ‡ãƒ¼ã‚¿ã¯ä¸Šæ›¸ãï¼‰"""
    filepath = os.path.join(DATA_DIR, "articles.csv")
    new_header = ["date", "note_id", "key", "title", "published_at", "created_at", "updated_at",
                  "age_days", "read_count", "like_count", "comment_count"]
    existing, old_header = _remove_rows_by_date(filepath, today)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®åˆ—é †ãŒç•°ãªã‚‹å ´åˆã¯æ–°ãƒ˜ãƒƒãƒ€ã§æ›¸ãç›´ã™ï¼ˆæ—¢å­˜è¡Œã¯åˆ—æ•°ãŒåˆã‚ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ç ´æ£„ã—ãªã„ï¼‰
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(new_header)
        for row in existing:
            writer.writerow(row)
        for note in articles:
            writer.writerow([
                today,
                note["id"],
                note["key"],
                note["name"],
                note.get("published_at", ""),
                note.get("created_at", ""),
                note.get("updated_at", ""),
                note.get("age_days", ""),
                note["read_count"],
                note["like_count"],
                note.get("comment_count", 0),
            ])

    print(f"  â†’ {filepath} ã«{len(articles)}è¡Œæ›¸ãè¾¼ã¿")


def save_daily_summary_csv(today, total_pv, total_like, total_comment, article_count, follower_count):
    """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ã‚’CSVã«ä¿å­˜ï¼ˆåŒæ—¥ãƒ‡ãƒ¼ã‚¿ã¯ä¸Šæ›¸ãï¼‰"""
    filepath = os.path.join(DATA_DIR, "daily_summary.csv")
    existing, header = _remove_rows_by_date(filepath, today)
    if header is None:
        header = ["date", "article_count", "total_pv", "total_like", "total_comment", "follower_count"]

    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        for row in existing:
            writer.writerow(row)
        writer.writerow([
            today,
            article_count,
            total_pv,
            total_like,
            total_comment,
            follower_count if follower_count is not None else "",
        ])

    print(f"  â†’ {filepath} ã«æ›¸ãè¾¼ã¿")


def main():
    print(f"=== note-stats-tracker ===")
    today = get_today_jst()
    print(f"æ—¥ä»˜: {today}")

    # Cookieæ¤œè¨¼
    validate_cookie()

    # CookieæœŸé™ãƒã‚§ãƒƒã‚¯
    check_cookie_expiry()

    # èªè¨¼ãƒã‚§ãƒƒã‚¯ï¼ˆstats APIã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‹äº‹å‰ç¢ºèªï¼‰
    verify_auth()

    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—
    print("\nğŸ“Š è¨˜äº‹ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
    articles, total_pv, total_like, total_comment = fetch_all_articles()

    # æ—¥æ™‚æƒ…å ±å–å¾—ï¼ˆv3 APIï¼‰
    print("\nğŸ“… æ—¥æ™‚æƒ…å ±ï¼ˆpublished_atç­‰ï¼‰å–å¾—ä¸­...")
    articles = fetch_note_dates(articles, today)

    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å–å¾—
    print("\nğŸ‘¥ ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å–å¾—ä¸­...")
    follower_count = fetch_follower_count()

    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    os.makedirs(DATA_DIR, exist_ok=True)

    print("\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    save_articles_csv(today, articles)
    save_daily_summary_csv(today, total_pv, total_like, total_comment, len(articles), follower_count)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n=== å®Œäº† ===")
    print(f"è¨˜äº‹æ•°: {len(articles)}")
    print(f"ç·PV: {total_pv}")
    print(f"ç·ã‚¹ã‚­: {total_like}")
    if follower_count is not None:
        print(f"ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼: {follower_count}")


if __name__ == "__main__":
    main()
