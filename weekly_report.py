#!/usr/bin/env python3
"""
é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆé€±æœ«ãƒã‚§ãƒƒã‚¯ #4ã€œ#8ï¼‰ã«å¯¾å¿œã—ãŸ
Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

ä½¿ã„æ–¹:
  python weekly_report.py --week 2026-02-17
  python weekly_report.py --week 2026-02-17 --articles data/articles.csv --categories data/article_categories.csv
  python weekly_report.py --week 2026-02-17 --out reports/

å¯¾è±¡é€±: æŒ‡å®šã—ãŸæœˆæ›œæ—¥ã‹ã‚‰æ—¥æ›œæ—¥ã¾ã§ï¼ˆæœˆã€œæ—¥ï¼‰
å‡ºåŠ›: weekly-report-YYYY-MM-DD.mdï¼ˆæœˆæ›œæ—¥ã®æ—¥ä»˜ï¼‰
"""

import argparse
import csv
import io
import os
import sys
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path


# === å®šæ•° ===

CATEGORY_NAMES = {
    "A": "è¨­è¨ˆæ€æƒ³",
    "B": "è©¦è¡ŒéŒ¯èª¤",
    "C": "ãƒã‚¦ãƒ„ãƒ¼",
    "D": "æŒ¯ã‚Šè¿”ã‚Š",
    "E": "ã‚­ãƒ£ãƒ©ç³»",
    "F": "åˆæœŸæ—¥è¨˜",
    "G": "ç‰¹åˆ¥æ ",
}

CATEGORY_ORDER = ["A", "B", "C", "D", "E", "F", "G"]

# æœˆé–“ã®ç†æƒ³ãƒãƒ©ãƒ³ã‚¹ï¼ˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ #10ã‚ˆã‚Šï¼‰
MONTHLY_IDEAL = {
    "A": (2, 3),
    "B": (5, 8),
    "C": (3, 4),
    "D": (4, 4),
    "E": (1, 2),
    "G": (0, 1),
}

WEEKDAY_JA = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]


# === ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===

def load_articles(path: str) -> list[dict]:
    """articles.csv ã‚’èª­ã¿è¾¼ã‚€ã€‚æ—¥æ¬¡ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå½¢å¼ã€‚"""
    rows = []
    with open(path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            row["read_count"] = int(row["read_count"])
            row["like_count"] = int(row["like_count"])
            row["comment_count"] = int(row["comment_count"])
            rows.append(row)
    return rows


def load_categories(path: str) -> dict:
    """article_categories.csv ã‚’èª­ã¿è¾¼ã¿ã€key -> {number, category, title, date} ã®dictã‚’è¿”ã™ã€‚"""
    cat_map = {}
    with open(path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            key = row["key"].strip()
            cat_map[key] = {
                "number": row["article_number"].strip(),
                "category": row["category"].strip(),
                "title": row["title"].strip(),
                "published_date": row.get("published_date", "").strip(),
            }
    return cat_map


def load_daily_summary(path: str) -> list[dict]:
    """daily_summary.csv ã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    rows = []
    if not os.path.exists(path):
        return rows
    with open(path, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


# === æœ€æ–°ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å–å¾— ===

def get_latest_snapshot(articles: list[dict], as_of_date: str = None) -> dict:
    """
    å„è¨˜äº‹ã®æœ€æ–°ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—ã€‚
    as_of_date ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ã€ãã®æ—¥ä»˜ä»¥å‰ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†ã€‚
    æˆ»ã‚Šå€¤: key -> {read_count, like_count, comment_count, title, ...}
    """
    latest = {}
    for row in articles:
        if as_of_date and row["date"] > as_of_date:
            continue
        key = row["key"]
        if key not in latest or row["date"] >= latest[key]["date"]:
            latest[key] = row
    return latest


# === é€±ã®è¨˜äº‹ã‚’ç‰¹å®š ===

def get_week_articles(cat_map: dict, week_start: str, week_end: str) -> list[dict]:
    """å¯¾è±¡é€±ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã‚’è¿”ã™ã€‚"""
    week_arts = []
    for key, info in cat_map.items():
        pub = info["published_date"]
        if pub and week_start <= pub <= week_end:
            week_arts.append({**info, "key": key})
    # article_number ã§ã‚½ãƒ¼ãƒˆï¼ˆpreãªã©éæ•°å€¤ã¯æœ«å°¾ï¼‰
    def sort_key(x):
        try:
            return (0, int(x["number"]))
        except ValueError:
            return (1, x["number"])
    week_arts.sort(key=sort_key)
    return week_arts


# === Î·è¨ˆç®— ===

def calc_eta(read_count: int, like_count: int) -> float | None:
    if read_count == 0:
        return None
    return like_count / read_count


# === ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===

def generate_report(
    week_start_str: str,
    articles: list[dict],
    cat_map: dict,
    daily_summary: list[dict],
) -> str:
    week_start = datetime.strptime(week_start_str, "%Y-%m-%d")
    week_end = week_start + timedelta(days=6)
    week_end_str = week_end.strftime("%Y-%m-%d")

    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥ã‚’ç‰¹å®š
    all_dates = sorted(set(r["date"] for r in articles))
    # å¯¾è±¡é€±æœ«ä»¥å‰ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿æ—¥
    valid_dates = [d for d in all_dates if d <= week_end_str]
    if not valid_dates:
        # é€±æœ«ä»¥å¾Œã®ãƒ‡ãƒ¼ã‚¿ã—ã‹ãªã„å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã®æœ€æ–°ã‚’ä½¿ã†
        data_date = all_dates[-1] if all_dates else week_end_str
    else:
        data_date = valid_dates[-1]

    snapshot = get_latest_snapshot(articles, data_date)
    week_arts = get_week_articles(cat_map, week_start_str, week_end_str)

    lines = []
    lines.append(f"# é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ {week_start_str}ã€œ{week_end_str}")
    lines.append(f"")
    lines.append(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append(f"ãƒ‡ãƒ¼ã‚¿åŸºæº–æ—¥: {data_date}")
    lines.append("")

    # --- #4: ã‚«ãƒ†ã‚´ãƒªãƒãƒ©ãƒ³ã‚¹ ---
    lines.append("---")
    lines.append("")
    lines.append("## 4. ä»Šé€±ã®ã‚«ãƒ†ã‚´ãƒªãƒãƒ©ãƒ³ã‚¹")
    lines.append("")

    if not week_arts:
        lines.append("ä»Šé€±ã®å…¬é–‹è¨˜äº‹: ãªã—ï¼ˆarticle_categories.csvã«å¯¾è±¡é€±ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
        lines.append("")
    else:
        cat_count = Counter(a["category"] for a in week_arts)
        ab_count = cat_count.get("A", 0) + cat_count.get("B", 0)

        lines.append(f"å…¬é–‹è¨˜äº‹æ•°: **{len(week_arts)}æœ¬**")
        lines.append("")

        # ãƒ†ãƒ¼ãƒ–ãƒ«
        lines.append("| # | ã‚«ãƒ†ã‚´ãƒª | ã‚¿ã‚¤ãƒˆãƒ« | å…¬é–‹æ—¥ |")
        lines.append("|---|----------|---------|--------|")
        for a in week_arts:
            cat_label = f"{a['category']}({CATEGORY_NAMES.get(a['category'], '?')})"
            dow = WEEKDAY_JA[datetime.strptime(a["published_date"], "%Y-%m-%d").weekday()]
            lines.append(f"| {a['number']} | {cat_label} | {a['title']} | {a['published_date']}({dow}) |")
        lines.append("")

        # ã‚«ãƒ†ã‚´ãƒªé›†è¨ˆ
        lines.append("ã‚«ãƒ†ã‚´ãƒªå†…è¨³: " + "ã€€".join(
            f"**{c}**: {cat_count.get(c, 0)}æœ¬" for c in CATEGORY_ORDER if cat_count.get(c, 0) > 0
        ))
        lines.append("")

        # åˆ¤å®š
        if ab_count >= 2:
            lines.append(f"âœ… A+B = {ab_count}æœ¬ï¼ˆä¸€æ¬¡æƒ…å ±ã‚¾ãƒ¼ãƒ³ç¶­æŒï¼‰")
        else:
            lines.append(f"âš ï¸ A+B = {ab_count}æœ¬ï¼ˆ2æœ¬æœªæº€ã€‚æ¥é€±ã¯A or Bã‚’å¢—ã‚„ã™ï¼‰")

        cd_count = cat_count.get("C", 0) + cat_count.get("D", 0)
        if cd_count > ab_count and ab_count < 2:
            lines.append(f"âš ï¸ C+D = {cd_count}æœ¬ > A+Bã€‚ãƒã‚¦ãƒ„ãƒ¼ãƒ»æŒ¯ã‚Šè¿”ã‚Šã«åã‚Šæ°—å‘³")

        e_count = cat_count.get("E", 0)
        if e_count > 2:
            lines.append(f"ğŸ“ E = {e_count}æœ¬ã€‚ã‚­ãƒ£ãƒ©ç³»ãŒå¤šã‚ï¼ˆæœˆ1ã€œ2æœ¬ãŒç›®å®‰ï¼‰")

    lines.append("")

    # --- #5: ã‚«ãƒ†ã‚´ãƒªåˆ¥Î·åºåˆ— ---
    lines.append("---")
    lines.append("")
    lines.append("## 5. ã‚«ãƒ†ã‚´ãƒªåˆ¥Î·ï¼ˆå…¨è¨˜äº‹ãƒ™ãƒ¼ã‚¹ï¼‰")
    lines.append("")

    # å…¨è¨˜äº‹ã®Î·ã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«é›†è¨ˆ
    cat_etas = defaultdict(list)
    for key, snap in snapshot.items():
        if key in cat_map:
            cat = cat_map[key]["category"]
            eta = calc_eta(snap["read_count"], snap["like_count"])
            if eta is not None:
                cat_etas[cat].append(eta)

    lines.append("| ã‚«ãƒ†ã‚´ãƒª | è¨˜äº‹æ•° | å¹³å‡Î· | ä¸­å¤®å€¤Î· | ä»®èª¬åºåˆ— |")
    lines.append("|----------|--------|-------|---------|----------|")

    cat_avg = {}
    for cat in CATEGORY_ORDER:
        etas = cat_etas.get(cat, [])
        if not etas:
            lines.append(f"| {cat}({CATEGORY_NAMES.get(cat, '')}) | 0 | - | - | |")
            continue
        avg = sum(etas) / len(etas)
        med = sorted(etas)[len(etas) // 2]
        cat_avg[cat] = avg
        lines.append(
            f"| {cat}({CATEGORY_NAMES.get(cat, '')}) | {len(etas)} | "
            f"{avg:.1%} | {med:.1%} | |"
        )
    lines.append("")

    # åºåˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆA > B > C,D > Eï¼‰
    def check_order(higher, lower):
        if higher in cat_avg and lower in cat_avg:
            return cat_avg[higher] >= cat_avg[lower]
        return None  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³

    order_checks = [
        ("A", "B"),
        ("B", "C"),
        ("B", "D"),
        ("C", "E"),
        ("D", "E"),
    ]

    violations = []
    for h, l in order_checks:
        result = check_order(h, l)
        if result is False:
            violations.append(
                f"âš ï¸ {h}({cat_avg.get(h, 0):.1%}) < {l}({cat_avg.get(l, 0):.1%})"
            )

    if not violations:
        lines.append("âœ… ä»®èª¬åºåˆ—ï¼ˆA > B > Cãƒ»D > Eï¼‰ã¯ç¶­æŒ")
    else:
        lines.append("åºåˆ—ã®å´©ã‚Œ:")
        for v in violations:
            lines.append(f"  {v}")
    lines.append("")

    # --- #6: ã‚¹ã‚­ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP20 ---
    lines.append("---")
    lines.append("")
    lines.append("## 6. ã‚¹ã‚­ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP20")
    lines.append("")

    # å…¨è¨˜äº‹ã®Î·ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    eta_ranking = []
    for key, snap in snapshot.items():
        eta = calc_eta(snap["read_count"], snap["like_count"])
        if eta is not None and snap["read_count"] >= 10:  # æœ€ä½PVãƒ•ã‚£ãƒ«ã‚¿
            cat_info = cat_map.get(key, {})
            eta_ranking.append({
                "key": key,
                "title": snap.get("title", cat_info.get("title", "?")),
                "number": cat_info.get("number", "?"),
                "category": cat_info.get("category", "?"),
                "eta": eta,
                "read_count": snap["read_count"],
                "like_count": snap["like_count"],
            })

    eta_ranking.sort(key=lambda x: x["eta"], reverse=True)
    top20 = eta_ranking[:20]

    lines.append("| é †ä½ | # | Cat | Î· | PV | ã‚¹ã‚­ | ã‚¿ã‚¤ãƒˆãƒ« |")
    lines.append("|------|---|-----|---|-----|------|---------|")
    for i, r in enumerate(top20, 1):
        lines.append(
            f"| {i} | {r['number']} | {r['category']} | "
            f"{r['eta']:.1%} | {r['read_count']} | {r['like_count']} | "
            f"{r['title'][:40]} |"
        )
    lines.append("")

    # TOP10ã®ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
    top10_cats = Counter(r["category"] for r in top20[:10])
    lines.append("TOP10ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ: " + "ã€€".join(
        f"**{c}**: {n}æœ¬" for c, n in sorted(top10_cats.items())
    ))

    ab_in_top10 = top10_cats.get("A", 0) + top10_cats.get("B", 0)
    cd_in_top10 = top10_cats.get("C", 0) + top10_cats.get("D", 0)
    e_in_top10 = top10_cats.get("E", 0)

    if ab_in_top10 >= 5:
        lines.append("âœ… TOP10ã«Aãƒ»BãŒå¤šã„ â†’ ä»®èª¬é€šã‚Š")
    elif cd_in_top10 > ab_in_top10:
        lines.append("ğŸ“ TOP10ã«Cãƒ»DãŒå¤šã„ â†’ å®Ÿç”¨ç³»ãŒåˆºã•ã£ã¦ã„ã‚‹æ™‚æœŸã€‚ã€Œãªãœã€ã‚’æ·±æ˜ã‚Šã™ã‚‹ãƒãƒ£ãƒ³ã‚¹")
    if e_in_top10 >= 2:
        lines.append("ğŸ“ EãŒãƒ©ãƒ³ã‚¯ã‚¤ãƒ³ â†’ ãƒ•ã‚¡ãƒ³å±¤ãŒè‚²ã£ã¦ã„ã‚‹å…†å€™")
    lines.append("")

    # --- #7: PVÃ—ã‚¹ã‚­ã®ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¾ãƒ¼ãƒ³ ---
    lines.append("---")
    lines.append("")
    lines.append("## 7. PVÃ—ã‚¹ã‚­ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¾ãƒ¼ãƒ³")
    lines.append("")

    # PVã¨ã‚¹ã‚­ã®ä¸­å¤®å€¤ã‚’åŸºæº–ã«4è±¡é™ã«åˆ†ã‘ã‚‹
    all_pvs = [s["read_count"] for s in snapshot.values() if s["read_count"] > 0]
    all_likes = [s["like_count"] for s in snapshot.values()]

    if all_pvs:
        pv_median = sorted(all_pvs)[len(all_pvs) // 2]
        like_median = sorted(all_likes)[len(all_likes) // 2]

        zones = {"å³ä¸Š(é«˜PVé«˜ã‚¹ã‚­)": [], "å³ä¸‹(é«˜PVä½ã‚¹ã‚­)": [], "å·¦ä¸Š(ä½PVé«˜ã‚¹ã‚­)": [], "å·¦ä¸‹(ä½PVä½ã‚¹ã‚­)": []}
        for key, snap in snapshot.items():
            if key not in cat_map:
                continue
            cat = cat_map[key]["category"]
            pv = snap["read_count"]
            like = snap["like_count"]
            if pv >= pv_median and like >= like_median:
                zone = "å³ä¸Š(é«˜PVé«˜ã‚¹ã‚­)"
            elif pv >= pv_median and like < like_median:
                zone = "å³ä¸‹(é«˜PVä½ã‚¹ã‚­)"
            elif pv < pv_median and like >= like_median:
                zone = "å·¦ä¸Š(ä½PVé«˜ã‚¹ã‚­)"
            else:
                zone = "å·¦ä¸‹(ä½PVä½ã‚¹ã‚­)"
            zones[zone].append(cat)

        lines.append(f"åŸºæº–: PVä¸­å¤®å€¤={pv_median}ã€€ã‚¹ã‚­ä¸­å¤®å€¤={like_median}")
        lines.append("")

        for zone_name, cats in zones.items():
            cat_dist = Counter(cats)
            dist_str = "ã€€".join(f"{c}:{n}" for c, n in sorted(cat_dist.items()))
            lines.append(f"**{zone_name}**: {dist_str}")

        # Aãƒ»BãŒå·¦ä¸‹ã«ã‚ã‚‹è¨˜äº‹ã‚’è­¦å‘Š
        lines.append("")
        ab_low = []
        for key, snap in snapshot.items():
            if key not in cat_map:
                continue
            cat = cat_map[key]["category"]
            if cat in ("A", "B") and snap["read_count"] < pv_median and snap["like_count"] < like_median:
                ab_low.append(f"  - #{cat_map[key]['number']} {cat_map[key]['title'][:35]}ï¼ˆPV:{snap['read_count']} ã‚¹ã‚­:{snap['like_count']}ï¼‰")

        if ab_low:
            lines.append("âš ï¸ Aãƒ»Bã§å·¦ä¸‹ã‚¾ãƒ¼ãƒ³ã®è¨˜äº‹ï¼ˆã‚¿ã‚¤ãƒˆãƒ« or ãƒ†ãƒ¼ãƒé¸ã³ã®èª²é¡Œï¼‰:")
            for line in ab_low[:5]:
                lines.append(line)
        else:
            lines.append("âœ… Aãƒ»Bã®è¨˜äº‹ã¯å·¦ä¸‹ã‚¾ãƒ¼ãƒ³ã«é›†ä¸­ã—ã¦ã„ãªã„")
    lines.append("")

    # --- #8: Aãƒ»Bè¨˜äº‹ã®ä¼¸ã³å‚¾å‘ ---
    lines.append("---")
    lines.append("")
    lines.append("## 8. Aãƒ»Bè¨˜äº‹ã®ä¼¸ã³å‚¾å‘ï¼ˆåˆæ—¥ vs æœ€æ–°ï¼‰")
    lines.append("")

    # å„Aãƒ»Bè¨˜äº‹ã«ã¤ã„ã¦ã€æœ€ã‚‚å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨æœ€æ–°ã‚’æ¯”è¼ƒ
    ab_keys = [key for key, info in cat_map.items() if info["category"] in ("A", "B")]

    ab_growth = []
    for key in ab_keys:
        key_rows = sorted(
            [r for r in articles if r["key"] == key],
            key=lambda x: x["date"]
        )
        if len(key_rows) < 2:
            continue
        first = key_rows[0]
        last = key_rows[-1]
        pv_growth = last["read_count"] - first["read_count"]
        like_growth = last["like_count"] - first["like_count"]
        days = (datetime.strptime(last["date"], "%Y-%m-%d") - datetime.strptime(first["date"], "%Y-%m-%d")).days
        if days == 0:
            continue
        ab_growth.append({
            "number": cat_map[key]["number"],
            "title": cat_map[key]["title"],
            "category": cat_map[key]["category"],
            "pv_growth": pv_growth,
            "like_growth": like_growth,
            "days": days,
            "pv_per_day": pv_growth / days,
            "first_pv": first["read_count"],
            "last_pv": last["read_count"],
        })

    ab_growth.sort(key=lambda x: x["pv_per_day"], reverse=True)

    if ab_growth:
        lines.append("| # | Cat | PVå¢—åˆ† | æ—¥æ•° | PV/æ—¥ | å‚¾å‘ | ã‚¿ã‚¤ãƒˆãƒ« |")
        lines.append("|---|-----|--------|------|-------|------|---------|")
        for g in ab_growth[:15]:
            trend = "ğŸ“ˆ" if g["pv_per_day"] >= 2.0 else "â¡ï¸" if g["pv_per_day"] >= 0.5 else "ğŸ“‰"
            lines.append(
                f"| {g['number']} | {g['category']} | +{g['pv_growth']} | "
                f"{g['days']}æ—¥ | {g['pv_per_day']:.1f} | {trend} | "
                f"{g['title'][:35]} |"
            )
        lines.append("")
        growing = [g for g in ab_growth if g["pv_per_day"] >= 2.0]
        flat = [g for g in ab_growth if g["pv_per_day"] < 0.5]
        if growing:
            lines.append(f"ğŸ“ˆ ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«è¨˜äº‹ï¼ˆPV/æ—¥â‰¥2.0ï¼‰: {len(growing)}æœ¬ â†’ ä¸€æ¬¡æƒ…å ±ã®å¼·ã•")
        if flat:
            lines.append(f"ğŸ“‰ åˆæ—¥å‹è¨˜äº‹ï¼ˆPV/æ—¥<0.5ï¼‰: {len(flat)}æœ¬ â†’ æ§‹é€ åŒ–ã®è¦‹ç›´ã—ä½™åœ°ã‚ã‚Š")
    else:
        lines.append("Aãƒ»Bè¨˜äº‹ã®ãƒ‡ãƒ¼ã‚¿ãŒ2æ—¥åˆ†ä»¥ä¸Šã‚ã‚Šã¾ã›ã‚“ã€‚")
    lines.append("")

    # --- æœˆé–“ã‚«ãƒ†ã‚´ãƒªæ¯”ç‡ï¼ˆå‚è€ƒï¼‰ ---
    lines.append("---")
    lines.append("")
    lines.append("## å‚è€ƒ: ç›´è¿‘30æ—¥ã®ã‚«ãƒ†ã‚´ãƒªæ¯”ç‡")
    lines.append("")

    month_start = (week_end - timedelta(days=29)).strftime("%Y-%m-%d")
    month_arts = [
        info for info in cat_map.values()
        if info["published_date"] and month_start <= info["published_date"] <= week_end_str
    ]
    month_cats = Counter(a["category"] for a in month_arts)

    lines.append(f"æœŸé–“: {month_start}ã€œ{week_end_str}ï¼ˆ{len(month_arts)}æœ¬ï¼‰")
    lines.append("")
    lines.append("| ã‚«ãƒ†ã‚´ãƒª | å®Ÿç¸¾ | ç†æƒ³ï¼ˆæœˆé–“ï¼‰ | åˆ¤å®š |")
    lines.append("|----------|------|-------------|------|")
    for cat in CATEGORY_ORDER:
        actual = month_cats.get(cat, 0)
        if cat in MONTHLY_IDEAL:
            lo, hi = MONTHLY_IDEAL[cat]
            if actual < lo:
                judge = "âš ï¸ å°‘ãªã„"
            elif actual > hi:
                judge = "ğŸ“ å¤šã‚"
            else:
                judge = "âœ…"
            lines.append(f"| {cat}({CATEGORY_NAMES[cat]}) | {actual} | {lo}ã€œ{hi} | {judge} |")
        else:
            lines.append(f"| {cat}({CATEGORY_NAMES[cat]}) | {actual} | - | |")
    lines.append("")

    # --- ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ¨ç§»ï¼ˆdaily_summaryã‹ã‚‰ï¼‰ ---
    if daily_summary:
        lines.append("---")
        lines.append("")
        lines.append("## å‚è€ƒ: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ¨ç§»")
        lines.append("")

        week_summaries = [
            d for d in daily_summary
            if week_start_str <= d["date"] <= week_end_str
        ]
        if week_summaries:
            first_f = int(week_summaries[0]["follower_count"])
            last_f = int(week_summaries[-1]["follower_count"])
            diff = last_f - first_f
            sign = "+" if diff >= 0 else ""
            lines.append(f"é€±åˆ: {first_f} â†’ é€±æœ«: {last_f}ï¼ˆ{sign}{diff}ï¼‰")
        else:
            # é€±å†…ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€å‰å¾Œã®æœ€è¿‘ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
            all_summaries = sorted(daily_summary, key=lambda x: x["date"])
            if all_summaries:
                latest = all_summaries[-1]
                lines.append(f"æœ€æ–°ãƒ‡ãƒ¼ã‚¿ï¼ˆ{latest['date']}ï¼‰: ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ {latest['follower_count']}")
        lines.append("")

    # --- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ ---
    lines.append("---")
    lines.append("")
    lines.append("## ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ¥é€±ã®è¨˜äº‹ã«å‘ã‘ã¦ï¼‰")
    lines.append("")

    actions = []

    # ã‚«ãƒ†ã‚´ãƒªãƒãƒ©ãƒ³ã‚¹ã‹ã‚‰ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if week_arts:
        cat_count = Counter(a["category"] for a in week_arts)
        ab_count = cat_count.get("A", 0) + cat_count.get("B", 0)
        if ab_count < 2:
            actions.append("æ¥é€±7æœ¬ã®ä¸­ã«A or Bã‚’2æœ¬ä»¥ä¸Šå…¥ã‚Œã‚‹")

    # Î·åºåˆ—ã‹ã‚‰ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if violations:
        actions.append("Î·åºåˆ—ãŒå´©ã‚Œã¦ã„ã‚‹ç®‡æ‰€ã‚ã‚Š â†’ è©²å½“ã‚«ãƒ†ã‚´ãƒªã®ç›´è¿‘è¨˜äº‹ã‚’ç¢ºèª")

    # æœˆé–“ãƒãƒ©ãƒ³ã‚¹ã‹ã‚‰ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    for cat in ["A", "B"]:
        if cat in MONTHLY_IDEAL:
            lo, _ = MONTHLY_IDEAL[cat]
            if month_cats.get(cat, 0) < lo:
                actions.append(f"{cat}({CATEGORY_NAMES[cat]})ãŒæœˆé–“ã§ä¸è¶³æ°—å‘³ â†’ æ„è­˜çš„ã«é…ç½®")

    if not actions:
        actions.append("ç¾çŠ¶ã®æ›¸ãæ–¹ã‚’ç¶™ç¶š")

    for a in actions:
        lines.append(f"- {a}")
    lines.append("")

    return "\n".join(lines)


# === ãƒ¡ã‚¤ãƒ³ ===

def main():
    parser = argparse.ArgumentParser(description="é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    parser.add_argument(
        "--week", required=True,
        help="å¯¾è±¡é€±ã®æœˆæ›œæ—¥ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--articles", default="data/articles.csv",
        help="articles.csvã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--categories", default="data/article_categories.csv",
        help="article_categories.csvã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--daily", default="data/daily_summary.csv",
        help="daily_summary.csvã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--out", default="reports/",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    args = parser.parse_args()

    # æ—¥ä»˜ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    try:
        week_start = datetime.strptime(args.week, "%Y-%m-%d")
    except ValueError:
        print(f"ã‚¨ãƒ©ãƒ¼: --week ã¯ YYYY-MM-DD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„: {args.week}", file=sys.stderr)
        sys.exit(1)

    if week_start.weekday() != 0:
        print(f"è­¦å‘Š: {args.week} ã¯æœˆæ›œæ—¥ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆ{WEEKDAY_JA[week_start.weekday()]}æ›œæ—¥ï¼‰ã€‚æœˆæ›œæ—¥ã«èª¿æ•´ã—ã¾ã™ã€‚")
        week_start = week_start - timedelta(days=week_start.weekday())
        print(f"èª¿æ•´å¾Œ: {week_start.strftime('%Y-%m-%d')}")

    week_start_str = week_start.strftime("%Y-%m-%d")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    articles = load_articles(args.articles)
    cat_map = load_categories(args.categories)
    daily_summary = load_daily_summary(args.daily)

    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
    print(f"  articles.csv: {len(articles)}è¡Œ")
    print(f"  article_categories.csv: {len(cat_map)}è¨˜äº‹")
    print(f"  daily_summary.csv: {len(daily_summary)}è¡Œ")
    print()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_report(week_start_str, articles, cat_map, daily_summary)

    # å‡ºåŠ›
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / f"weekly-report-{week_start_str}.md"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {out_path}")
    print()
    print(report)


if __name__ == "__main__":
    main()
